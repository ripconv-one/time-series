# -*- coding: utf-8 -*-
"""Data Analytics Project - Forecasting the impact of natural gas in the overall fuel mix

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1p9iS_oW0x5hKKQPmjxIIoTMR5KkalUKL
"""

import pandas as pd
from sklearn.preprocessing import MinMaxScaler
import numpy as np
import matplotlib as plt
import seaborn as sns
import sklearn
import plotly.express as px
from tabulate import tabulate
from sklearn.metrics import r2_score

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_squared_error

df = pd.read_excel("/content/ISO_Com_Data.xlsx")

df.head()

df.describe()

df.shape

count_table = []
for item in df.columns:
    count_table.append([item, df[item].count()])

print("Count of non-null values:")
print(tabulate(count_table, headers=['Column', 'Count'], tablefmt='pretty'))
print('\n')

# Number of unique values
unique_table = []
for item in df.columns:
    unique_table.append([item, df[item].nunique()])

print("Number of unique values:")
print(tabulate(unique_table, headers=['Column', 'Unique Count'], tablefmt='pretty'))
print('\n')

# Data types
dtype_table = []
for item in df.columns:
    dtype_table.append([item, df[item].dtype])

print("Data types:")
print(tabulate(dtype_table, headers=['Column', 'Data Type'], tablefmt='pretty'))
print('\n')

# null
null_table = []
for item in df.columns:
    null_table.append([item, df[item].isnull().sum()])

print("Null values:")
print(tabulate(null_table, headers=['Column', 'NULL sum'], tablefmt='pretty'))

#sns.pairplot(df)

"""#PrecProcessing

#Removing unwanted columns and NULL Values

##Removing unwanted cols
"""

df.columns

selected_columns =['interval_end_utc', 'fuel_mix.coal', 'fuel_mix.landfill_gas', 'fuel_mix.nuclear', 'fuel_mix.oil', 'fuel_mix.other', 'fuel_mix.refuse', 'fuel_mix.wood', 'btm_solar.btm_solar']
df_drop=df.drop(columns=selected_columns)
df_drop

"""##Remove Null Vals"""

wf = df_drop.dropna()
wf

"""##Transform data"""

scaler = MinMaxScaler()
columns_to_scale = ['net_load', 'renewables','renewables_to_load_ratio', 'load.load', 'load_forecast.load_forecast',
       'fuel_mix.hydro', 'fuel_mix.natural_gas', 'fuel_mix.solar',
       'fuel_mix.wind']

wf[columns_to_scale] = scaler.fit_transform(wf[columns_to_scale])

wf

"""###DATE AND TIME"""

wf['interval_start_utc'] = pd.to_datetime(df['interval_start_utc'])
x= wf['date'] = wf['interval_start_utc'].dt.date
y=wf['time'] = wf['interval_start_utc'].dt.time
x

y

wf

"""
#Nornammilty"""

import pandas as pd
from scipy.stats import shapiro, normaltest
import matplotlib.pyplot as plt

# Assuming your data is stored in a DataFrame named df
# Extract the target variable for normality tests
target_variable = wf['fuel_mix.natural_gas']

# Visualize the distribution using a histogram
plt.hist(target_variable, bins='auto', alpha=0.7, color='blue', edgecolor='black')
plt.title('Histogram of Natural Gas')
plt.xlabel('Value')
plt.ylabel('Frequency')
plt.show()

# Statistical tests for normality
shapiro_test_statistic, shapiro_p_value = shapiro(target_variable)
print(f"Shapiro-Wilk Test: Statistic = {shapiro_test_statistic}, p-value = {shapiro_p_value}")

normaltest_test_statistic, normaltest_p_value = normaltest(target_variable)
print(f"Normality Test: Statistic = {normaltest_test_statistic}, p-value = {normaltest_p_value}")

variables = wf[['net_load', 'fuel_mix.natural_gas','renewables', 'renewables_to_load_ratio', 'load.load',
                 'load_forecast.load_forecast', 'fuel_mix.hydro', 'fuel_mix.solar', 'fuel_mix.wind']]


variables.hist(bins='auto', figsize=(12, 10))
plt.suptitle('Histograms of  Variables', y=0.94)
plt.show()

for column in variables.columns:
    test_statistic, p_value = shapiro(variables[column])
    print(f"{column}: Shapiro-Wilk Test - Statistic = {test_statistic}, p-value = {p_value}")

    test_statistic, p_value = normaltest(variables[column])
    print(f"{column}: Normality Test - Statistic = {test_statistic}, p-value = {p_value}\n")

import pandas as pd
import seaborn as sns
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

# Assuming your data is stored in a DataFrame named df
# Extract relevant columns for correlation analysis
relevant_columns = ['net_load', 'renewables', 'renewables_to_load_ratio', 'load.load',
                    'load_forecast.load_forecast', 'fuel_mix.hydro', 'fuel_mix.solar', 'fuel_mix.wind', 'fuel_mix.natural_gas']
relevant_data = wf[relevant_columns]

# Calculate the correlation matrix
correlation_matrix = relevant_data.corr()

# Visualize the correlation matrix using a heatmap
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix')
plt.show()

# Plot autocorrelation and partial autocorrelation functions for the target variable
plot_acf(df['fuel_mix.natural_gas'])
plt.title('Autocorrelation Function (ACF) for fuel_mix.natural_gas')
plt.show()

plot_pacf(df['fuel_mix.natural_gas'])
plt.title('Partial Autocorrelation Function (PACF) for fuel_mix.natural_gas')
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Assuming your data is in a CSV file named 'energy_data.csv'
# You can read the data into a pandas DataFrame


# 1. Descriptive Statistics
descriptive_stats = wf.describe()

# 2. Data Distribution - Histograms
wf.hist(figsize=(12, 10))
plt.suptitle('Histograms of Numerical Variables')
plt.show()

# 3. Data Types
data_types = wf.dtypes

# 4. Missing Values
missing_values = wf.isnull().sum()

# 5. Outliers - Box Plots
wf.boxplot(figsize=(12, 8))
plt.title('Box Plot of Numerical Variables')
plt.show()

# 6. Data Structure
data_structure = wf.shape

# 7. Correlations
correlation_matrix = wf.corr()

# 8. Summary Metrics
summary_metrics = {
    'Mean Net Load': wf['net_load'].mean(),
    'Median Renewables to Load Ratio': wf['renewables_to_load_ratio'].median()
}

# Display Results
print("1. Descriptive Statistics:")
print(descriptive_stats)

print("\n2. Data Distribution:")
# (Histograms are already displayed)

print("\n3. Data Types:")
print(data_types)

print("\n4. Missing Values:")
print(missing_values)

print("\n5. Outliers:")
# (Box plot is already displayed)

print("\n6. Data Structure:")
print("Number of Observations (Rows) and Variables (Columns):", data_structure)

print("\n7. Correlations:")
print(correlation_matrix)

print("\n8. Summary Metrics:")
print(summary_metrics)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from tabulate import tabulate


# 1. Descriptive Statistics
descriptive_stats = wf.describe()

# 2. Data Types
data_types = pd.DataFrame(wf.dtypes, columns=['Data Type'])

# 4. Missing Values
missing_values = pd.DataFrame(wf.isnull().sum(), columns=['Missing Values'])

# 6. Data Structure
data_structure = pd.DataFrame({'Number of Observations (Rows) and Variables (Columns)': [wf.shape]})

# 7. Correlations
correlation_matrix = wf.corr()

# 8. Summary Metrics
summary_metrics = pd.DataFrame({
    'Metric': ['Mean Net Load', 'Median Renewables to Load Ratio'],
    'Value': [wf['net_load'].mean(), wf['renewables_to_load_ratio'].median()]
})

# Display Results
print("\n1. Descriptive Statistics:")
print(descriptive_stats)

print("\n2. Data Types:")
print(tabulate(data_types, headers='keys', tablefmt='pretty'))

print("\n4. Missing Values:")
print(tabulate(missing_values, headers='keys', tablefmt='pretty'))

print("\n6. Data Structure:")
print(tabulate(data_structure, headers='keys', tablefmt='pretty'))

print("\n7. Correlations:")
print(correlation_matrix)

print("\n8. Summary Metrics:")
print(tabulate(summary_metrics, headers='keys', tablefmt='pretty'))

import scipy.stats as stats
data_group1 = wf['net_load']
data_group2 = wf['fuel_mix.natural_gas']

# Perform independent t-test
t_statistic, p_value = stats.ttest_ind(data_group1, data_group2)

# Print the results
print(f'T-statistic: {t_statistic}')
print(f'P-value: {p_value}')
# Check the significance level (e.g., 0.05)
alpha = 0.05

# Compare p-value to the significance level
if p_value < alpha:
    print("There is a significant difference between the groups.")
else:
    print("Fail to reject the null hypothesis - There is no significant difference between the groups.")

import statsmodels.api as sm
import pandas as pd

# Assuming you have a DataFrame 'df' with columns 'independent_variable' and 'dependent_variable'
# Replace these with your actual column names

# Example data (replace these with your actual data)


# Add a constant term to the independent variable
X = sm.add_constant(wf['fuel_mix.natural_gas'])

# Fit the OLS model
model = sm.OLS(wf['net_load'], X)
results = model.fit()

# Print the regression results
print(results.summary())

# Perform independent t-test
t_statistic, p_value = stats.ttest_ind(data_group1, data_group2)

# Print the results
print(f'\nT-statistic: {t_statistic}')
print(f'P-value: {p_value}')
# Check the significance level (e.g., 0.05)
alpha = 0.05

import pandas as pd
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestRegressor



# Assuming your target variable is 'net_load' and features are other columns
X = wf.drop('fuel_mix.natural_gas', axis=1)
y = wf['net_load']

# Instantiate a machine learning model (e.g., RandomForestRegressor)
model = RandomForestRegressor()

# Perform cross-validation
# Here, 'neg_mean_squared_error' is used as the scoring metric, but you can choose others based on your problem
# The 'cv' parameter determines the number of folds (e.g., 5 for 5-fold cross-validation)
cv_scores = cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=5)

# Display cross-validation results
print("Cross-Validation Scores:")
print(cv_scores)
print("Mean Cross-Validation Score:", np.mean(cv_scores))

wf.columns

import pandas as pd
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestRegressor

selected_columns =['interval_start_utc','date', 'time']
jf = wf.drop(columns=selected_columns)
# Assuming your target variable is 'net_load' and features are other columns
X = jf.drop('net_load', axis=1)
y = jf['net_load']

# Instantiate a machine learning model (e.g., RandomForestRegressor)
model = RandomForestRegressor()

# Perform cross-validation with error_score='raise'
try:
    # Here, 'neg_mean_squared_error' is used as the scoring metric, but you can choose others based on your problem
    # The 'cv' parameter determines the number of folds (e.g., 5 for 5-fold cross-validation)
    cv_scores = cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=5, error_score='raise')

    # Display cross-validation results
    print("Cross-Validation Scores:")
    print(cv_scores)
    print("Mean Cross-Validation Score:", np.mean(cv_scores))
except Exception as e:
    print("An error occurred during cross-validation:", e)

"""#Regression"""

wf.columns



X_columns = ['interval_start_utc', 'net_load', 'renewables',
       'renewables_to_load_ratio', 'load.load', 'load_forecast.load_forecast',
       'fuel_mix.hydro', 'fuel_mix.natural_gas', 'fuel_mix.solar',
       'fuel_mix.wind', 'date', 'time']
Y_column = ['interval_start_utc', 'net_load', 'renewables',
       'renewables_to_load_ratio', 'load.load', 'load_forecast.load_forecast',
       'fuel_mix.hydro', 'fuel_mix.natural_gas', 'fuel_mix.solar',
       'fuel_mix.wind', 'date', 'time']

X_columns[1]

# Choose the columns for X (features) and Y (target)
X_columns = ['net_load']
Y_column = 'renewables'

# Extract X and Y from the DataFrame
X = wf[X_columns]
Y = wf[Y_column]
#X2 = ['0.99872']
# Split the data into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Create a linear regression model
model = LinearRegression()

# Train the model on the training set
model.fit(X_train, Y_train)

# Make predictions on the test set
Y_pred = model.predict(X_test)
#Y_pred2 = model.predict(X2)
# Evaluate the model
mse = mean_squared_error(Y_test, Y_pred)
print(f'Mean Squared Error: {mse}')

r2 = r2_score(Y_test, Y_pred)
print(f'R-squared: {r2}')


# Create DataFrames for the training and testing sets with predictions
train_df = pd.DataFrame({X_columns[0]: X_train[X_columns[0]], Y_column: Y_train, 'Predicted': model.predict(X_train), 'Set': 'Train'})
test_df = pd.DataFrame({X_columns[0]: X_test[X_columns[0]], Y_column: Y_test, 'Predicted': Y_pred, 'Set': 'Test'})

# Concatenate the DataFrames
result_df = pd.concat([train_df, test_df])

# Plot the scatter plot with regression line using Plotly Express
fig = px.scatter(result_df, x=X_columns[0], y=Y_column, color='Set',
                 title='Linear Regression - Renewables vs Net_load',
                 labels={'Predicted': 'Predicted Values', Y_column: 'Renewables'})

# Add the regression line to the plot
fig.add_scatter(x=result_df[X_columns[0]], y=result_df['Predicted'], mode='lines', line=dict(dash='solid'), name='Regression Line')

# Show the plot
fig.show()

import statsmodels.api as sm
#fit linear regression model
model = sm.OLS(Y, X).fit()

#view model summary
print(model.summary())

# Choose the columns for X (features) and Y (target)
X_columns = ['net_load']
Y_column = 'load.load'

# Extract X and Y from the DataFrame
X = wf[X_columns]
Y = wf[Y_column]
X2 = ['0.99872']
# Split the data into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Create a linear regression model
model = LinearRegression()

# Train the model on the training set
model.fit(X_train, Y_train)

# Make predictions on the test set
Y_pred = model.predict(X_test)
#Y_pred2 = model.predict(X2)
# Evaluate the model
mse = mean_squared_error(Y_test, Y_pred)
print(f'Mean Squared Error: {mse}')

r2 = r2_score(Y_test, Y_pred)
print(f'R-squared: {r2}')
# Create DataFrames for the training and testing sets with predictions
train_df = pd.DataFrame({X_columns[0]: X_train[X_columns[0]], Y_column: Y_train, 'Predicted': model.predict(X_train), 'Set': 'Train'})
test_df = pd.DataFrame({X_columns[0]: X_test[X_columns[0]], Y_column: Y_test, 'Predicted': Y_pred, 'Set': 'Test'})

# Concatenate the DataFrames
result_df = pd.concat([train_df, test_df])

# Plot the scatter plot with regression line using Plotly Express
fig = px.scatter(result_df, x=X_columns[0], y=Y_column, color='Set',
                 title='Linear Regression - Actual Load vs Net_load',
                 labels={'Predicted': 'Predicted Values', Y_column: 'Actual Load'})

# Add the regression line to the plot
fig.add_scatter(x=result_df[X_columns[0]], y=result_df['Predicted'], mode='lines', line=dict(dash='dash'), name='Regression Line')

# Show the plot
fig.show()

# Choose the columns for X (features) and Y (target)
X_columns = ['net_load']
Y_column = 'fuel_mix.natural_gas'

# Extract X and Y from the DataFrame
X = wf[X_columns]
Y = wf[Y_column]

# Split the data into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Create a linear regression model
model = LinearRegression()

# Train the model on the training set
model.fit(X_train, Y_train)

# Make predictions on the test set
Y_pred = model.predict(X_test)
#Y_pred2 = model.predict(X2)
# Evaluate the model
mse = mean_squared_error(Y_test, Y_pred)
print(f'Mean Squared Error: {mse}')
r2 = r2_score(Y_test, Y_pred)
print(f'R-squared: {r2}')
# Create DataFrames for the training and testing sets with predictions
train_df = pd.DataFrame({X_columns[0]: X_train[X_columns[0]], Y_column: Y_train, 'Predicted': model.predict(X_train), 'Set': 'Train'})
test_df = pd.DataFrame({X_columns[0]: X_test[X_columns[0]], Y_column: Y_test, 'Predicted': Y_pred, 'Set': 'Test'})

# Concatenate the DataFrames
result_df = pd.concat([train_df, test_df])

# Plot the scatter plot with regression line using Plotly Express
fig = px.scatter(result_df, x=X_columns[0], y=Y_column, color='Set',
                 title='Linear Regression - fuel_mix.natural_gas vs net_load',
                 labels={'Predicted': 'Predicted Values', Y_column: 'fuel_mix.natural_gas'})

# Add the regression line to the plot
fig.add_scatter(x=result_df[X_columns[0]], y=result_df['Predicted'], mode='lines', line=dict(dash='dash'), name='Regression Line')

# Show the plot
fig.show()

import pandas as pd
import plotly.express as px
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_squared_error

# Choose the columns for X (features) and Y (target)
X_columns = ['net_load']
Y_column = 'fuel_mix.natural_gas'

# Extract X and Y from the DataFrame
X = wf[X_columns]
Y = wf[Y_column]

# Split the data into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Create Polynomial features
degree = 8  # You can change the degree as needed
poly_features = PolynomialFeatures(degree=degree)
X_train_poly = poly_features.fit_transform(X_train)
X_test_poly = poly_features.transform(X_test)

# Create a linear regression model
model = LinearRegression()

# Train the model on the polynomial features
model.fit(X_train_poly, Y_train)

# Make predictions on the test set
Y_pred = model.predict(X_test_poly)
# Create DataFrames for the training and testing sets with predictions
train_df = pd.DataFrame({X_columns[0]: X_train[X_columns[0]], Y_column: Y_train, 'Predicted': model.predict(X_train_poly), 'Set': 'Train'})
test_df = pd.DataFrame({X_columns[0]: X_test[X_columns[0]], Y_column: Y_test, 'Predicted': Y_pred, 'Set': 'Test'})

# Concatenate the DataFrames
result_df = pd.concat([train_df, test_df])

# Plot the scatter plot with regression line using Plotly Express
fig = px.scatter(result_df, x=X_columns[0], y=Y_column, color='Set',
                 title=f'Polynomial Regression (Degree {degree}) - fuel_mix.natural_gas vs net_load',
                 labels={'Predicted': 'Predicted Values', Y_column: 'fuel_mix.natural_ga'})

# Sort the values for better visualization
result_df = result_df.sort_values(by=X_columns[0])
# Add the regression line to the plot
fig.add_scatter(x=result_df[X_columns[0]], y=model.predict(poly_features.transform(result_df[X_columns[0]].values.reshape(-1, 1))),
                mode='lines', line=dict(dash='solid'), name='Regression Line')

# Show the plot
fig.show()
# Evaluate the model
mse = mean_squared_error(Y_test, Y_pred)
print(f'Mean Squared Error: {mse}')
r2 = r2_score(Y_test, Y_pred)
print(f'R-squared: {r2}')




"""# Calculate residuals
residuals = Y_test - Y_pred

# Plot a histogram of residuals
fig_hist = px.histogram(x=residuals, nbins=50, title='Residuals Histogram')
fig_hist.show()

# Check for normality using a Q-Q plot
fig_qq = px.scatter(x=np.sort(residuals), y=np.sort(np.random.normal(0, 1, len(residuals))),
                    title='Q-Q Plot for Residuals', labels={'x': 'Ordered Residuals', 'y': 'Theoretical Quantiles'})
fig_qq.add_shape(type='line', line=dict(dash='dash'), x0=np.min(residuals), x1=np.max(residuals), y0=np.min(residuals), y1=np.max(residuals))
fig_qq.show()"""
